import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import joblib
from pathlib import Path

# configs
N_EPOCHS = 500
BATCH_SIZE = 64
LR = 1e-3
PRINT_EVERY = 50
SEED = 42

torch.manual_seed(SEED)
np.random.seed(SEED)

ROOT = Path(__file__).resolve().parents[2]
df = pd.read_csv(ROOT / 'data' / 'hf_data.csv')

df['logRe'] = np.log10(df['Re'])

# low fid models
xgb_CL = joblib.load(ROOT / 'models' / 'low fidelity' / 'lf_CLtot.joblib')
xgb_CD = joblib.load(ROOT / 'models' / 'low fidelity' / 'lf_CDtot.joblib')


BASE_FEATS = ['Aspect','Taper','Sweep','Dihedral','Twist','Alpha','logRe']
X_base = df[BASE_FEATS].values.astype(np.float32)

CL_LF = xgb_CL.predict(X_base).astype(np.float32)
CD_LF = np.clip(xgb_CD.predict(X_base), 1e-10, None).astype(np.float32)

X = np.column_stack([X_base, CL_LF, CD_LF]).astype(np.float32)


CL_HF = df['CLtot_HF'].values.astype(np.float32).reshape(-1,1)
LD_HF = df['L_D_HF'].values.astype(np.float32).reshape(-1,1)

logCD_HF = np.log(np.clip(df['CDtot_HF'].values, 1e-10, None))
logCD_LF = np.log(CD_LF)
dlogCD = (logCD_HF - logCD_LF).reshape(-1,1).astype(np.float32)

# split train test
(
    X_train, X_val,
    CL_train, CL_val,
    dlogCD_train, dlogCD_val,
    LD_train, LD_val,
    CD_LF_train, CD_LF_val
) = train_test_split(
    X, CL_HF, dlogCD, LD_HF, CD_LF,
    test_size=0.2,
    random_state=SEED
)

# normalisation
CL_mean, CL_std = CL_train.mean(), CL_train.std()
LD_mean, LD_std = LD_train.mean(), LD_train.std()
dlogCD_mean, dlogCD_std = dlogCD_train.mean(), dlogCD_train.std()

CL_train = (CL_train - CL_mean) / CL_std
CL_val   = (CL_val   - CL_mean) / CL_std

LD_train = (LD_train - LD_mean) / LD_std
LD_val   = (LD_val   - LD_mean) / LD_std

dlogCD_train = (dlogCD_train - dlogCD_mean) / dlogCD_std
dlogCD_val   = (dlogCD_val   - dlogCD_mean) / dlogCD_std


train_loader = DataLoader(
    TensorDataset(
        torch.from_numpy(X_train),
        torch.from_numpy(CL_train),
        torch.from_numpy(dlogCD_train),
        torch.from_numpy(LD_train),
    ),
    batch_size=BATCH_SIZE,
    shuffle=True
)

val_loader = DataLoader(
    TensorDataset(
        torch.from_numpy(X_val),
        torch.from_numpy(CL_val),
        torch.from_numpy(dlogCD_val),
        torch.from_numpy(LD_val),
    ),
    batch_size=BATCH_SIZE
)

class AeroMultiNN(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.backbone = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        self.head_CL = nn.Linear(64, 1)
        self.head_CD = nn.Linear(64, 1)   # dlogCD
        self.head_LD = nn.Linear(64, 1)

    def forward(self, x):
        z = self.backbone(x)
        return self.head_CL(z), self.head_CD(z), self.head_LD(z)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = AeroMultiNN(X.shape[1]).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=LR)
mse = nn.MSELoss()

# training
train_hist, val_hist = [], []

for epoch in range(N_EPOCHS):
    model.train()
    losses = []

    for xb, clb, dcb, ldb in train_loader:
        xb, clb, dcb, ldb = xb.to(device), clb.to(device), dcb.to(device), ldb.to(device)

        optimizer.zero_grad()
        p_cl, p_dc, p_ld = model(xb)

        loss = (
            mse(p_cl, clb)
            + mse(p_dc, dcb)
            + mse(p_ld, ldb)
        )

        loss.backward()
        optimizer.step()
        losses.append(loss.item())

    train_loss = np.mean(losses)
    train_hist.append(train_loss)

    model.eval()
    with torch.no_grad():
        vloss = []
        for xb, clb, dcb, ldb in val_loader:
            xb, clb, dcb, ldb = xb.to(device), clb.to(device), dcb.to(device), ldb.to(device)
            p_cl, p_dc, p_ld = model(xb)
            vloss.append(
                mse(p_cl, clb).item()
                + mse(p_dc, dcb).item()
                + mse(p_ld, ldb).item()
            )

    val_loss = np.mean(vloss)
    val_hist.append(val_loss)

    if epoch % PRINT_EVERY == 0 or epoch == N_EPOCHS - 1:
        print(f'Epoch {epoch:4d} | Train {train_loss:.4e} | Val {val_loss:.4e}')


model.eval()
with torch.no_grad():
    X_tensor = torch.from_numpy(X).to(device)
    p_cl, p_dc, p_ld = model(X_tensor)

CL_pred = p_cl.cpu().numpy() * CL_std + CL_mean
LD_pred = p_ld.cpu().numpy() * LD_std + LD_mean

dlogCD_pred = p_dc.cpu().numpy() * dlogCD_std + dlogCD_mean
CD_pred = np.exp(np.log(CD_LF) + dlogCD_pred.flatten())

# accuracy
print('\nAccuracy score')
print(f'CL: MAE={mean_absolute_error(CL_HF, CL_pred):.4f}, R2={r2_score(CL_HF, CL_pred):.4f}')
print(f'CD: MAE={mean_absolute_error(df['CDtot_HF'], CD_pred):.4f}, R2={r2_score(df['CDtot_HF'], CD_pred):.4f}')
print(f'L/D: MAE={mean_absolute_error(LD_HF, LD_pred):.4f}, R2={r2_score(LD_HF, LD_pred):.4f}')

plt.figure(figsize=(6,4))
plt.plot(train_hist, label='Train')
plt.plot(val_hist, label='Val')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training History')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1,3, figsize=(15,4))
for ax, (y_true, y_pred, name) in zip(
    axes,
    [
        (CL_HF.flatten(), CL_pred.flatten(), 'CL'),
        (df['CDtot_HF'].values, CD_pred, 'CD'),
        (LD_HF.flatten(), LD_pred.flatten(), 'L/D'),
    ]
):
    ax.scatter(y_true, y_pred, alpha=0.6)
    lims = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]
    ax.plot(lims, lims, 'r--')
    ax.set_title(name)
    ax.set_xlabel('True')
    ax.set_ylabel('Predicted')
    ax.grid(True)

plt.tight_layout()
plt.show()

MODEL_PATH = ROOT / 'models' / 'neural network' / 'trained_model.pt'
torch.save(model.state_dict(), MODEL_PATH)
print(f'Saved trained NN to {MODEL_PATH}')

import pickle
norm_stats = {
    'CL_mean': CL_mean, 'CL_std': CL_std,
    'dlogCD_mean': dlogCD_mean, 'dlogCD_std': dlogCD_std,
    'LD_mean': LD_mean, 'LD_std': LD_std
}
with open(ROOT / 'models' / 'norm_stats.pkl', 'wb') as f:
    pickle.dump(norm_stats, f)
print(f'Saved normalization stats to {ROOT / 'models' / 'neural network' / 'norm_stats.pkl'}')
